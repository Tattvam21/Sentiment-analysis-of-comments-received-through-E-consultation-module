{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9eadf0fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q transformers scikit-learn torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "426a7c65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers>=4.44.0 in /opt/anaconda3/lib/python3.13/site-packages (4.56.2)\n",
      "Requirement already satisfied: accelerate>=0.26.0 in /opt/anaconda3/lib/python3.13/site-packages (1.10.1)\n",
      "Requirement already satisfied: torch in /opt/anaconda3/lib/python3.13/site-packages (2.8.0)\n",
      "Requirement already satisfied: scikit-learn in /opt/anaconda3/lib/python3.13/site-packages (1.7.2)\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/lib/python3.13/site-packages (from transformers>=4.44.0) (3.17.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /opt/anaconda3/lib/python3.13/site-packages (from transformers>=4.44.0) (0.35.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/anaconda3/lib/python3.13/site-packages (from transformers>=4.44.0) (2.1.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/anaconda3/lib/python3.13/site-packages (from transformers>=4.44.0) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/anaconda3/lib/python3.13/site-packages (from transformers>=4.44.0) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/anaconda3/lib/python3.13/site-packages (from transformers>=4.44.0) (2024.11.6)\n",
      "Requirement already satisfied: requests in /opt/anaconda3/lib/python3.13/site-packages (from transformers>=4.44.0) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /opt/anaconda3/lib/python3.13/site-packages (from transformers>=4.44.0) (0.22.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /opt/anaconda3/lib/python3.13/site-packages (from transformers>=4.44.0) (0.6.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/anaconda3/lib/python3.13/site-packages (from transformers>=4.44.0) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/anaconda3/lib/python3.13/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers>=4.44.0) (2025.3.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/anaconda3/lib/python3.13/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers>=4.44.0) (4.12.2)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /opt/anaconda3/lib/python3.13/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers>=4.44.0) (1.1.10)\n",
      "Requirement already satisfied: psutil in /opt/anaconda3/lib/python3.13/site-packages (from accelerate>=0.26.0) (5.9.0)\n",
      "Requirement already satisfied: setuptools in /opt/anaconda3/lib/python3.13/site-packages (from torch) (72.1.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /opt/anaconda3/lib/python3.13/site-packages (from torch) (1.13.3)\n",
      "Requirement already satisfied: networkx in /opt/anaconda3/lib/python3.13/site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /opt/anaconda3/lib/python3.13/site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: scipy>=1.8.0 in /opt/anaconda3/lib/python3.13/site-packages (from scikit-learn) (1.15.3)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /opt/anaconda3/lib/python3.13/site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /opt/anaconda3/lib/python3.13/site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/anaconda3/lib/python3.13/site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/lib/python3.13/site-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/lib/python3.13/site-packages (from requests->transformers>=4.44.0) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/lib/python3.13/site-packages (from requests->transformers>=4.44.0) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/lib/python3.13/site-packages (from requests->transformers>=4.44.0) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.13/site-packages (from requests->transformers>=4.44.0) (2025.8.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -U \"transformers>=4.44.0\" \"accelerate>=0.26.0\" torch scikit-learn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d375635d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, numpy as np, pandas as pd, re\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, get_linear_schedule_with_warmup\n",
    "from torch.optim import AdamW\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a89ce6f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment_text</th>\n",
       "      <th>clean_text</th>\n",
       "      <th>gold_label</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>who asked for this? lol nah ðŸ‘Ž #redtape</td>\n",
       "      <td>who asked for this? lol nah ðŸ‘Ž &lt;HASHTAG&gt;</td>\n",
       "      <td>negative</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Love this update ðŸ”¥ less red tape #progress</td>\n",
       "      <td>love this update ðŸ”¥ less red tape &lt;HASHTAG&gt;</td>\n",
       "      <td>positive</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>This amendment simplifies compliance procedure...</td>\n",
       "      <td>this amendment simplifies compliance procedure...</td>\n",
       "      <td>positive</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>bruh these fines are wild ðŸ¤¦ #overkill</td>\n",
       "      <td>bruh these fines are wild ðŸ¤¦ &lt;HASHTAG&gt;</td>\n",
       "      <td>negative</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>bruh these fines are wild ðŸ¤¦ #overkill</td>\n",
       "      <td>bruh these fines are wild ðŸ¤¦ &lt;HASHTAG&gt;</td>\n",
       "      <td>negative</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        comment_text  \\\n",
       "0             who asked for this? lol nah ðŸ‘Ž #redtape   \n",
       "1         Love this update ðŸ”¥ less red tape #progress   \n",
       "2  This amendment simplifies compliance procedure...   \n",
       "3              bruh these fines are wild ðŸ¤¦ #overkill   \n",
       "4              bruh these fines are wild ðŸ¤¦ #overkill   \n",
       "\n",
       "                                          clean_text gold_label  label  \n",
       "0            who asked for this? lol nah ðŸ‘Ž <HASHTAG>   negative      0  \n",
       "1         love this update ðŸ”¥ less red tape <HASHTAG>   positive      2  \n",
       "2  this amendment simplifies compliance procedure...   positive      2  \n",
       "3              bruh these fines are wild ðŸ¤¦ <HASHTAG>   negative      0  \n",
       "4              bruh these fines are wild ðŸ¤¦ <HASHTAG>   negative      0  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load your file\n",
    "df = pd.read_csv(\"sih_mixedtypes_english_100k.csv\")\n",
    "\n",
    "def clean_text(s):\n",
    "    if not isinstance(s, str): return \"\"\n",
    "    s = s.lower().strip()\n",
    "    s = re.sub(r\"http\\S+|www\\S+\", \" <URL> \", s)\n",
    "    s = re.sub(r\"@\\w+\", \" <USER> \", s)\n",
    "    s = re.sub(r\"#\\w+\", \" <HASHTAG> \", s)\n",
    "    s = re.sub(r\"\\s+\", \" \", s)\n",
    "    return s.strip()\n",
    "\n",
    "df[\"clean_text\"] = df[\"comment_text\"].apply(clean_text)\n",
    "\n",
    "labels = sorted(df[\"gold_label\"].unique().tolist())\n",
    "label2id = {l:i for i,l in enumerate(labels)}\n",
    "id2label = {i:l for l,i in label2id.items()}\n",
    "df[\"label\"] = df[\"gold_label\"].map(label2id)\n",
    "\n",
    "idx = np.arange(len(df))\n",
    "idx_train, idx_test = train_test_split(idx, test_size=0.2, stratify=df[\"label\"], random_state=42)\n",
    "train_df = df.loc[idx_train].reset_index(drop=True)\n",
    "test_df  = df.loc[idx_test].reset_index(drop=True)\n",
    "\n",
    "df[[\"comment_text\",\"clean_text\",\"gold_label\",\"label\"]].head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "495c0ef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"distilbert-base-uncased\"  # faster on Mac; change to bert-base-uncased if you want\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "MAX_LEN = 160\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "class CommentDS(Dataset):\n",
    "    def __init__(self, texts, labels):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    def __getitem__(self, idx):\n",
    "        enc = tokenizer(\n",
    "            self.texts[idx],\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=MAX_LEN,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        item = {k: v.squeeze(0) for k, v in enc.items()}\n",
    "        item[\"labels\"] = torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        return item\n",
    "\n",
    "train_ds = CommentDS(train_df[\"clean_text\"].tolist(), train_df[\"label\"].tolist())\n",
    "test_ds  = CommentDS(test_df[\"clean_text\"].tolist(),  test_df[\"label\"].tolist())\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader  = DataLoader(test_ds,  batch_size=BATCH_SIZE, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8ffca674",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: mps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else (\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
    "print(\"Device:\", device)\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    MODEL_NAME, num_labels=len(labels), id2label=id2label, label2id=label2id\n",
    ").to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7c6d52f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 2\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "num_training_steps = EPOCHS * len(train_loader)\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer, num_warmup_steps=int(0.1 * num_training_steps), num_training_steps=num_training_steps\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6217e944",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
